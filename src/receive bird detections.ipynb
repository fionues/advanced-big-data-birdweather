{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f90675a-f347-4ae0-a174-3206be4feac4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstreaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StreamingContext\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m(timeout, func, window):\n\u001b[32m      5\u001b[39m   \u001b[38;5;66;03m# Create a local StreamingContext and SparkContext.\u001b[39;00m\n\u001b[32m      6\u001b[39m   \u001b[38;5;66;03m# In the databricks environment a default SparkContext already exists so we can't create a second one.\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def main(timeout, func, window):\n",
    "  # Create a local StreamingContext and SparkContext.\n",
    "  # In the databricks environment a default SparkContext already exists so we can't create a second one.\n",
    "  sc = SparkContext(\"local[2]\", \"Streaming-Exercise\")\n",
    "  ssc = StreamingContext(sc, window)\n",
    "  stream = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "  \n",
    "  func(stream)\n",
    "  try:\n",
    "    ssc.start()             # Start the computation\n",
    "    ssc.awaitTerminationOrTimeout(timeout)  # To see the output in Python we have to wait until it's finished.\n",
    "  except Exception as e:\n",
    "    print(str(e))\n",
    "  finally:\n",
    "    ssc.stop(False)\n",
    "\n",
    "\n",
    "def print_stream(stream):\n",
    "  # TODO - Print the Stream (one line of code)\n",
    "  stream.pprint()\n",
    "\n",
    "# Let it run (start first the netcat Notebook to see some output)\n",
    "main(10,print_stream,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7b1f6-ca9a-41b8-b43f-c8d96e3e132f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e9ff63-292e-4591-9207-72c54f8875cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- algorithm: string (nullable = true)\n",
      " |-- certainty: string (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- metadata: string (nullable = true)\n",
      " |-- probability: double (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- soundscape: struct (nullable = true)\n",
      " |    |-- endTime: double (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- mode: string (nullable = true)\n",
      " |    |-- startTime: double (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |-- species: struct (nullable = true)\n",
      " |    |-- color: string (nullable = true)\n",
      " |    |-- commonName: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- imageUrl: string (nullable = true)\n",
      " |    |-- pngUrl: string (nullable = true)\n",
      " |    |-- scientificName: string (nullable = true)\n",
      " |    |-- thumbnailUrl: string (nullable = true)\n",
      " |-- stationId: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Stream aus json file\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Variante 1: Erstellen eines lokalen StreamingContext und SparkContext. [Nur zum Testen!]\n",
    "sc = SparkContext(\"local\", \"StructStreaming-Client-greguali\")\n",
    "\n",
    "# Initialisieren der Spark-Sitzung\n",
    "spark = SparkSession.builder.appName(\"StructStreaming-Client-greguali\").getOrCreate()\n",
    "\n",
    "tmpdirname = \"bird-detections\"\n",
    "\n",
    "# Sicherstellen, dass der Pfad zur JSON-Datei korrekt ist\n",
    "json_file_path = f\"bird_detections_example.json\"\n",
    "\n",
    "# Lesen der JSON-Datei in ein DataFrame\n",
    "df = spark.read.json(json_file_path)\n",
    "\n",
    "# Abrufen des Schemas des DataFrame\n",
    "json_schema = df.schema\n",
    "\n",
    "# Anzeigen des Schemas\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9767e75-38e5-4c0b-9cd9-e4caf13acf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDF(streamingDF):\n",
    "  iter = 0\n",
    "  while iter < 10:\n",
    "    if(streamingDF.count() > 0):\n",
    "      print(\"Number of entries in dataframe: \"+ str(streamingDF.count()))\n",
    "      streamingDF.show(20, False) # the parameter False prevents Spark from truncating the output\n",
    "      iter += 1\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d72c13a-793b-4e17-88dc-99e0a587a363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the stream from the /tmp/tweets folder (change to json stream)\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "# with or without file: prefix\n",
    "inputPath = tmpdirname\n",
    "\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .schema(json_schema)\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "\n",
    "streamingInputDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023ade29-c334-4e62-8bae-623b25bc2ba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `detections`.`stationId` cannot be resolved. Did you mean one of the following? [`stationId`, `timestamp`, `certainty`, `metadata`, `species`].;\n'Project ['detections.stationId, 'detections.species.id]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@13fc4ff9,json,List(),Some(StructType(StructField(algorithm,StringType,true),StructField(certainty,StringType,true),StructField(confidence,DoubleType,true),StructField(id,LongType,true),StructField(lat,DoubleType,true),StructField(lon,DoubleType,true),StructField(metadata,StringType,true),StructField(probability,DoubleType,true),StructField(score,DoubleType,true),StructField(soundscape,StructType(StructField(endTime,DoubleType,true),StructField(id,LongType,true),StructField(mode,StringType,true),StructField(startTime,DoubleType,true),StructField(url,StringType,true)),true),StructField(species,StructType(StructField(color,StringType,true),StructField(commonName,StringType,true),StructField(id,LongType,true),StructField(imageUrl,StringType,true),StructField(pngUrl,StringType,true),StructField(scientificName,StringType,true),StructField(thumbnailUrl,StringType,true)),true),StructField(stationId,LongType,true),StructField(timestamp,StringType,true))),List(),None,Map(path -> bird-detections),None), FileSource[bird-detections], [algorithm#34, certainty#35, confidence#36, id#37L, lat#38, lon#39, metadata#40, probability#41, score#42, soundscape#43, species#44, stationId#45L, timestamp#46]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m streamingETLQuery \u001b[38;5;241m=\u001b[39m \u001b[43mstreamingInputDF\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdetections.stationId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdetections.species.id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectionstream\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      9\u001b[0m streamingDF \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect count(*) as count, id from detectionstream group by id order by count desc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m printDF(streamingDF)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `detections`.`stationId` cannot be resolved. Did you mean one of the following? [`stationId`, `timestamp`, `certainty`, `metadata`, `species`].;\n'Project ['detections.stationId, 'detections.species.id]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@13fc4ff9,json,List(),Some(StructType(StructField(algorithm,StringType,true),StructField(certainty,StringType,true),StructField(confidence,DoubleType,true),StructField(id,LongType,true),StructField(lat,DoubleType,true),StructField(lon,DoubleType,true),StructField(metadata,StringType,true),StructField(probability,DoubleType,true),StructField(score,DoubleType,true),StructField(soundscape,StructType(StructField(endTime,DoubleType,true),StructField(id,LongType,true),StructField(mode,StringType,true),StructField(startTime,DoubleType,true),StructField(url,StringType,true)),true),StructField(species,StructType(StructField(color,StringType,true),StructField(commonName,StringType,true),StructField(id,LongType,true),StructField(imageUrl,StringType,true),StructField(pngUrl,StringType,true),StructField(scientificName,StringType,true),StructField(thumbnailUrl,StringType,true)),true),StructField(stationId,LongType,true),StructField(timestamp,StringType,true))),List(),None,Map(path -> bird-detections),None), FileSource[bird-detections], [algorithm#34, certainty#35, confidence#36, id#37L, lat#38, lon#39, metadata#40, probability#41, score#42, soundscape#43, species#44, stationId#45L, timestamp#46]\n"
     ]
    }
   ],
   "source": [
    "streamingETLQuery = streamingInputDF \\\n",
    "  .select(\"stationId\", \"species.id\")\\\n",
    "  .writeStream \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"detectionstream\") \\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()\n",
    "  \n",
    "streamingDF = spark.sql(\"select count(*) as count, id from detectionstream group by id order by count desc\")\n",
    "\n",
    "printDF(streamingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15611e15-5d31-4dca-acf4-867081bd2c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:00:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:00:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:00:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:00:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:00:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:00:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:01:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:01:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:01:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:01:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-18 15:01:04\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aktueller Code!:\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "\n",
    "def main(timeout, func, window):\n",
    "\n",
    "    sc = SparkContext(\"local\", \"Streaming-Exercise\")\n",
    "    ssc = StreamingContext(sc, window)\n",
    "    stream = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "    \n",
    "    func(stream)\n",
    "    try:\n",
    "        ssc.start()             \n",
    "        ssc.awaitTerminationOrTimeout(timeout)  \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    finally:\n",
    "        ssc.stop(False)\n",
    "\n",
    "def print_json_stream(stream):\n",
    "    \n",
    "    stream.map(lambda x: json.loads(x.decode('utf-8'))).pprint()\n",
    "\n",
    "\n",
    "main(10, print_json_stream, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cb2d06-f160-4985-b767-4c333c45d5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
